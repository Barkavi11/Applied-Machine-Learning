# -*- coding: utf-8 -*-
"""Data Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A2A14KQM0oATXN4R8lHOBXzB4qCnKqpE

**Data Cleaning**
"""

import pandas as pd
import numpy as np

# Sample dataset with missing values and duplicates
data1 = pd.DataFrame({
    'ID': [1, 2, 3, 4, 4],
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'David'],
    'Age': [25, 30, np.nan, 40, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Houston']
})

print("Original Data:")
print(data1)

# Remove duplicate rows
data1 = data1.drop_duplicates()

# Fill missing values in 'Age' with the column mean
data1['Age'].fillna(data1['Age'].mean(), inplace=True)

print("\nCleaned Data:")
print(data1)

"""**Data Integration**"""

# Second dataset to merge
data2 = pd.DataFrame({
    'ID': [1, 2, 3, 5],
    'Income': [50000, 60000, 55000, 45000],
    'Gender': ['F', 'M', 'M', 'F']
})

# Merge datasets on 'ID'
merged_data = pd.merge(data1, data2, on='ID', how='left')

print("Merged Data:")
print(merged_data)

"""**Data Transformation**"""

# Encode categorical variable 'Gender' using one-hot encoding
merged_data = pd.get_dummies(merged_data, columns=['Gender'], drop_first=True)

# Normalize 'Income' column using Min-Max scaling
merged_data['Income_Normalized'] = (
    (merged_data['Income'] - merged_data['Income'].min()) /
    (merged_data['Income'].max() - merged_data['Income'].min())
)

print("Transformed Data:")
print(merged_data)

"""**Data Reduction**"""

# Drop columns that are not useful for analysis
reduced_data = merged_data.drop(columns=['City', 'Income'])

print("Reduced Final Data:")
print(reduced_data)

"""**Upload the Excel File to Google Colab**"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Load the Excel file into a DataFrame
df = pd.read_excel("student_data.xlsx")

# Display the data
print(df)

df.head()

"""**Data Cleaning**"""

# Make a copy of the dataset for cleaning
cleaned_df = df.copy()

# Remove duplicate rows
cleaned_df = cleaned_df.drop_duplicates()

# Fill missing values
cleaned_df['Age'] = cleaned_df['Age'].fillna(cleaned_df['Age'].mean())
cleaned_df['Gender'] = cleaned_df['Gender'].fillna(cleaned_df['Gender'].mode()[0])
cleaned_df['Grade'] = cleaned_df['Grade'].fillna(cleaned_df['Grade'].mode()[0])
cleaned_df['City'] = cleaned_df['City'].fillna(cleaned_df['City'].mode()[0])
cleaned_df['EnrollmentDate'] = cleaned_df['EnrollmentDate'].fillna(pd.Timestamp('2023-01-01'))

# View cleaned data
cleaned_df.head()

"""**Data Integration**"""

# Create simulated additional data for merging
import random

additional_data = pd.DataFrame({
    'StudentID': cleaned_df['StudentID'].sample(frac=0.8).values,
    'Club': np.random.choice(['Science', 'Art', 'Sports', 'None'], size=int(0.8 * len(cleaned_df)))
})

# Merge on StudentID
merged_df = pd.merge(cleaned_df, additional_data, on='StudentID', how='left')

# View merged data
merged_df.head()

"""**Data Transformation**"""

# Make a copy for transformation
transformed_df = merged_df.copy()

# One-hot encode categorical columns
transformed_df = pd.get_dummies(transformed_df, columns=['Gender', 'Grade', 'Club'], drop_first=True)

# Normalize Age (Min-Max Scaling)
transformed_df['Age_Normalized'] = (
    (transformed_df['Age'] - transformed_df['Age'].min()) /
    (transformed_df['Age'].max() - transformed_df['Age'].min())
)

# Convert EnrollmentDate to datetime
transformed_df['EnrollmentDate'] = pd.to_datetime(transformed_df['EnrollmentDate'])

# View transformed data
transformed_df.head()

"""**Data Reduction**"""

# Drop irrelevant or less useful columns
reduced_df = transformed_df.drop(columns=['Email', 'City', 'EnrollmentDate'])

# Final preprocessed dataset
reduced_df.head()